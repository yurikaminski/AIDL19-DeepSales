{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "import librosa\n",
    "import lib.util as ut\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model we want to make inferences with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='./trained_models/model_paper/model_25epochs.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To divide audio files. When making inferences over whole audio files, we pass interval_step=sample_rate/2 to produce 50% overlapping chunks of 3 second audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_audio_file(path, intervals_seconds, interval_step, sample_rate=8000):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # loads file and converts to the specified sample rate    \n",
    "    audio, fs = librosa.load(path, sample_rate)\n",
    "    \n",
    "    audio_array = []\n",
    "    for i in range(0, audio.shape[0], interval_step):   \n",
    "        interval = audio[i:i+sample_rate*intervals_seconds]\n",
    "#         print(\"interval from {} to {}\".format(i, i+sample_rate*intervals_seconds))\n",
    "        \n",
    "        # if the last interval is shorter han the interval in seconds we define we are going to ignore it\n",
    "        if interval.shape[0] < sample_rate*intervals_seconds:\n",
    "            break\n",
    "        else:\n",
    "            if (not ut.is_silence(interval,thresold_samples=0.70)):\n",
    "                audio_array.append(interval)\n",
    "            else:\n",
    "                print(\"Omitting chunk with silences in file {}\".format(path))\n",
    "\n",
    "    return np.array(audio_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Like the original create_tf_records but creating numpy array's instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_from_raw(audio_path, file_names_df, num_samples):\n",
    "    \n",
    "    x_data = []\n",
    "    labels = []\n",
    "    tf_record_files = 1  \n",
    "    for file in os.listdir(audio_path):\n",
    "\n",
    "        file_path = audio_path + \"/\" + file\n",
    "        short_name = file.split(\".\")[0]\n",
    "\n",
    "        # if the file is in the dataframe with the file names(train or test) we divide it, if not we ignore\n",
    "        if short_name in file_names_df[\"0\"].values:\n",
    "            print(\"reading file {}\".format(file))\n",
    "\n",
    "            divided_file =  divide_audio_file(file_path, intervals_seconds, sample_rate)\n",
    "\n",
    "            file_label = file_names_df[file_names_df[\"0\"] == short_name][\"class\"].values[0]\n",
    "            labels_array = np.ones(divided_file.shape[0]) * file_label\n",
    "            \n",
    "            x_data.extend(divided_file)\n",
    "            labels.extend(labels_array)            \n",
    "            \n",
    "            if len(x_data) > num_samples:\n",
    "                tf_record_files += 1\n",
    "                break;\n",
    "                        \n",
    "        else:\n",
    "            print(\"file {} not in the dataframe\".format(file))\n",
    "       \n",
    "                 \n",
    "    return (x_data,labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## makes inference of a whole audio by dividing it into overlapping (50%) chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_audio(file_name,label):\n",
    "    file_path = audio_path + \"/\" + file_name\n",
    "    audio_chunks=divide_audio_file(file_path, intervals_seconds, int(sample_rate*1.5))\n",
    "    audio_chunks = audio_chunks.reshape([audio_chunks.shape[0],intervals_seconds * sample_rate,1])\n",
    "    labels_array = np.ones(audio_chunks.shape[0]) * label\n",
    "    predictions = model.predict(audio_chunks)\n",
    "#     print(labels_array.shape)\n",
    "#     print(audio_chunks.shape)\n",
    "#     vc = pd.DataFrame(predictions)[0].apply(lambda x: 0 if x < 0.5 else 1).value_counts()\n",
    "#     if (vc[0]<vc[1]):\n",
    "#         res = 1\n",
    "#     else:\n",
    "#         res = 0\n",
    "        \n",
    "    return (predictions)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get string labels from numeric ones to build a confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabels(x):\n",
    "    if (x>=0.5):\n",
    "        label = \"CY\"\n",
    "    else:\n",
    "        label = \"CN\"\n",
    "    return(label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin of the execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:74: UserWarning: The `MaxoutDense` layer is deprecated and will be removed after 06/2017.\n",
      "  warnings.warn('The `MaxoutDense` layer is deprecated '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 24000, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24000, 64)         8320      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3000, 64)          0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 3000, 64, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3000, 64, 96)      5952      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 375, 64, 96)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 375, 64, 128)      430208    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 93, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 93, 64, 160)       348320    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 64, 160)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 1, 64, 160)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "maxout_dense_1 (MaxoutDense) (None, 128)               5243392   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "maxout_dense_2 (MaxoutDense) (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,069,281\n",
      "Trainable params: 6,069,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(model_path)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"./data/audiodata\"\n",
    "labels_file = \"./data/conflictlevel.csv\"\n",
    "\n",
    "intervals_seconds = 3\n",
    "sample_rate = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(labels_file, header=None)\n",
    "labels_df[\"class\"] = labels_df[1].apply(lambda x: 0 if x < 0 else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(\"./data/train_files.csv\")\n",
    "test_df = pd.read_csv(\"./data/test_files.csv\")\n",
    "# validation_df = pd.read_csv(\"./data/validation_files.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-05-10_2250_2280</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-09-13_2310_2340</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-05-16_1440_1470</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07-09-17_720_750</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06-11-08_1410_1440</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0    1  class\n",
       "0  06-05-10_2250_2280  7.9      1\n",
       "1  06-09-13_2310_2340  8.5      1\n",
       "2  07-05-16_1440_1470 -5.9      0\n",
       "3    07-09-17_720_750  1.1      1\n",
       "4  06-11-08_1410_1440  7.4      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting for file 06-05-10_2250_2280.wav\n",
      "predicting for file 06-09-13_2310_2340.wav\n",
      "predicting for file 07-05-16_1440_1470.wav\n",
      "predicting for file 07-09-17_720_750.wav\n",
      "predicting for file 06-11-08_1410_1440.wav\n",
      "predicting for file 06-05-10_1500_1530.wav\n",
      "predicting for file 06-12-13_1680_1710.wav\n",
      "predicting for file 06-12-20_1410_1440.wav\n",
      "predicting for file 08-01-09_1500_1530.wav\n",
      "predicting for file 07-02-21_1770_1800.wav\n",
      "predicting for file 07-02-28_390_420.wav\n",
      "predicting for file 06-04-12_1110_1140.wav\n",
      "predicting for file 06-11-08_720_750.wav\n",
      "predicting for file 06-10-11_2040_2070.wav\n",
      "predicting for file 06-10-18_570_600.wav\n",
      "predicting for file 06-09-06_1260_1290.wav\n",
      "predicting for file 08-01-23_1200_1230.wav\n",
      "predicting for file 06-12-13_1620_1650.wav\n",
      "predicting for file 06-12-13_270_300.wav\n",
      "predicting for file 07-02-14_390_420.wav\n",
      "predicting for file 07-01-31_1440_1470.wav\n",
      "predicting for file 07-01-31_1470_1500.wav\n",
      "predicting for file 06-10-18_1290_1320.wav\n",
      "predicting for file 07-09-17_240_270.wav\n",
      "predicting for file 07-03-28_1170_1200.wav\n",
      "predicting for file 08-01-09_1590_1620.wav\n",
      "predicting for file 06-05-10_420_450.wav\n",
      "predicting for file 06-11-08_450_480.wav\n",
      "predicting for file 06-05-24_1650_1680.wav\n",
      "predicting for file 06-11-29_2280_2310.wav\n",
      "predicting for file 07-01-31_1770_1800.wav\n",
      "predicting for file 06-11-15_1110_1140.wav\n",
      "predicting for file 06-05-03_1140_1170.wav\n",
      "predicting for file 06-05-31_240_270.wav\n",
      "predicting for file 07-11-14_1230_1260.wav\n",
      "predicting for file 06-10-04_1830_1860.wav\n",
      "predicting for file 06-09-06_990_1020.wav\n",
      "predicting for file 06-09-20_1080_1110.wav\n",
      "predicting for file 06-04-05_2280_2310.wav\n",
      "predicting for file 07-02-14_870_900.wav\n",
      "predicting for file 08-01-30_690_720.wav\n",
      "predicting for file 07-05-30_2280_2310.wav\n",
      "predicting for file 07-05-02_1740_1770.wav\n",
      "predicting for file 06-12-13_390_420.wav\n",
      "predicting for file 06-05-24_660_690.wav\n",
      "predicting for file 06-09-27_720_750.wav\n",
      "predicting for file 07-03-28_810_840.wav\n",
      "predicting for file 06-04-05_2070_2100.wav\n",
      "predicting for file 06-10-04_1410_1440.wav\n",
      "predicting for file 08-01-15_2040_2070.wav\n",
      "predicting for file 06-11-08_480_510.wav\n",
      "predicting for file 07-02-28_1050_1080.wav\n",
      "predicting for file 07-02-21_750_780.wav\n",
      "predicting for file 07-05-30_1080_1110.wav\n",
      "predicting for file 08-01-30_1680_1710.wav\n",
      "predicting for file 06-05-03_270_300.wav\n",
      "predicting for file 06-09-13_1530_1560.wav\n",
      "predicting for file 06-10-11_450_480.wav\n",
      "predicting for file 07-10-31_960_990.wav\n",
      "predicting for file 08-01-30_900_930.wav\n",
      "predicting for file 06-06-07_2700_2730.wav\n",
      "predicting for file 06-10-04_720_750.wav\n",
      "predicting for file 07-11-14_240_270.wav\n",
      "predicting for file 07-05-30_1230_1260.wav\n",
      "predicting for file 06-05-03_1860_1890.wav\n",
      "predicting for file 07-02-21_1740_1770.wav\n",
      "predicting for file 06-04-12_1830_1860.wav\n",
      "predicting for file 07-06-20_2010_2040.wav\n",
      "predicting for file 06-12-13_2160_2190.wav\n",
      "predicting for file 07-12-19_390_420.wav\n",
      "predicting for file 06-11-08_1800_1830.wav\n",
      "predicting for file 06-11-08_2130_2160.wav\n",
      "predicting for file 06-05-10_2190_2220.wav\n",
      "predicting for file 07-09-17_1020_1050.wav\n",
      "predicting for file 08-01-15_720_750.wav\n",
      "predicting for file 06-06-07_2220_2250.wav\n",
      "predicting for file 07-01-31_2100_2130.wav\n",
      "predicting for file 07-02-14_2280_2310.wav\n",
      "predicting for file 06-12-20_540_570.wav\n",
      "predicting for file 06-11-08_510_540.wav\n",
      "predicting for file 07-01-31_720_750.wav\n",
      "predicting for file 06-11-08_1110_1140.wav\n",
      "predicting for file 06-09-27_1920_1950.wav\n",
      "predicting for file 07-12-19_840_870.wav\n",
      "predicting for file 06-06-07_2310_2340.wav\n",
      "predicting for file 07-11-14_1830_1860.wav\n",
      "predicting for file 07-11-14_1320_1350.wav\n",
      "predicting for file 08-02-06_840_870.wav\n",
      "predicting for file 06-10-04_1140_1170.wav\n",
      "predicting for file 06-05-03_930_960.wav\n",
      "predicting for file 06-09-06_1470_1500.wav\n",
      "predicting for file 06-09-27_150_180.wav\n",
      "predicting for file 07-02-28_1950_1980.wav\n",
      "predicting for file 06-10-11_1200_1230.wav\n",
      "predicting for file 08-01-30_1470_1500.wav\n",
      "predicting for file 06-05-31_1440_1470.wav\n",
      "predicting for file 06-11-15_2070_2100.wav\n",
      "predicting for file 06-05-17_1590_1620.wav\n",
      "predicting for file 07-03-28_150_180.wav\n",
      "predicting for file 06-11-08_2010_2040.wav\n",
      "predicting for file 06-09-06_1440_1470.wav\n",
      "predicting for file 06-09-27_240_270.wav\n",
      "predicting for file 06-05-24_600_630.wav\n",
      "predicting for file 07-06-20_1680_1710.wav\n",
      "predicting for file 08-01-15_930_960.wav\n",
      "predicting for file 06-05-10_2370_2400.wav\n",
      "predicting for file 06-11-08_960_990.wav\n",
      "predicting for file 07-01-31_2280_2310.wav\n",
      "predicting for file 06-05-31_150_180.wav\n",
      "predicting for file 06-09-13_1650_1680.wav\n",
      "predicting for file 06-12-13_2250_2280.wav\n",
      "predicting for file 07-05-16_570_600.wav\n",
      "predicting for file 07-01-31_1350_1380.wav\n",
      "predicting for file 06-09-27_540_570.wav\n",
      "predicting for file 06-10-11_900_930.wav\n",
      "predicting for file 06-10-04_1320_1350.wav\n",
      "predicting for file 07-09-17_750_780.wav\n",
      "predicting for file 07-01-31_1800_1830.wav\n",
      "predicting for file 07-01-17_930_960.wav\n",
      "predicting for file 06-12-20_990_1020.wav\n",
      "predicting for file 07-05-16_990_1020.wav\n",
      "predicting for file 06-05-10_1440_1470.wav\n",
      "predicting for file 08-01-15_840_870.wav\n",
      "predicting for file 08-01-30_1650_1680.wav\n",
      "predicting for file 07-02-21_180_210.wav\n",
      "predicting for file 07-05-02_960_990.wav\n",
      "predicting for file 06-12-13_840_870.wav\n",
      "predicting for file 07-01-31_840_870.wav\n",
      "predicting for file 08-01-30_1080_1110.wav\n",
      "predicting for file 06-05-24_1620_1650.wav\n",
      "predicting for file 06-05-03_1740_1770.wav\n",
      "predicting for file 06-05-10_540_570.wav\n",
      "predicting for file 07-06-20_1500_1530.wav\n",
      "predicting for file 06-11-08_1350_1380.wav\n",
      "predicting for file 06-09-06_1950_1980.wav\n",
      "predicting for file 06-12-20_1110_1140.wav\n",
      "predicting for file 07-01-17_1050_1080.wav\n",
      "predicting for file 06-04-12_750_780.wav\n",
      "predicting for file 06-10-18_2340_2370.wav\n",
      "predicting for file 07-01-17_1800_1830.wav\n",
      "predicting for file 07-05-23_1500_1530.wav\n",
      "predicting for file 06-10-11_1590_1620.wav\n",
      "predicting for file 07-02-28_480_510.wav\n",
      "predicting for file 07-03-28_570_600.wav\n",
      "predicting for file 06-11-22_780_810.wav\n",
      "predicting for file 06-05-10_990_1020.wav\n",
      "predicting for file 06-09-13_1140_1170.wav\n",
      "predicting for file 07-02-28_780_810.wav\n",
      "predicting for file 06-12-20_1830_1860.wav\n",
      "predicting for file 06-12-06_480_510.wav\n",
      "predicting for file 06-05-10_2130_2160.wav\n",
      "predicting for file 06-05-17_2280_2310.wav\n",
      "predicting for file 08-01-23_840_870.wav\n",
      "predicting for file 06-04-05_2310_2340.wav\n",
      "predicting for file 06-12-20_1020_1050.wav\n",
      "predicting for file 07-03-28_870_900.wav\n",
      "predicting for file 07-05-02_1350_1380.wav\n",
      "predicting for file 06-05-24_780_810.wav\n",
      "predicting for file 07-05-02_1440_1470.wav\n",
      "predicting for file 06-04-12_210_240.wav\n",
      "predicting for file 07-10-31_2250_2280.wav\n",
      "predicting for file 07-02-28_750_780.wav\n",
      "predicting for file 07-06-20_690_720.wav\n",
      "predicting for file 07-01-17_2040_2070.wav\n",
      "predicting for file 07-05-30_1920_1950.wav\n",
      "predicting for file 07-05-16_1800_1830.wav\n",
      "predicting for file 06-09-06_330_360.wav\n",
      "predicting for file 06-04-05_1320_1350.wav\n",
      "predicting for file 07-05-23_840_870.wav\n",
      "predicting for file 06-11-08_360_390.wav\n",
      "predicting for file 06-04-05_240_270.wav\n",
      "predicting for file 07-10-31_2040_2070.wav\n",
      "predicting for file 07-05-16_1110_1140.wav\n",
      "predicting for file 07-02-14_960_990.wav\n",
      "predicting for file 07-11-28_720_750.wav\n",
      "predicting for file 06-12-20_1290_1320.wav\n",
      "predicting for file 06-11-08_1530_1560.wav\n",
      "predicting for file 07-02-14_1410_1440.wav\n",
      "predicting for file 06-05-10_2400_2430.wav\n",
      "predicting for file 08-01-23_1530_1560.wav\n",
      "predicting for file 06-11-15_540_570.wav\n",
      "predicting for file 07-02-28_1620_1650.wav\n",
      "predicting for file 07-05-16_450_480.wav\n",
      "predicting for file 06-10-18_390_420.wav\n",
      "predicting for file 07-11-14_480_510.wav\n",
      "predicting for file 07-09-17_450_480.wav\n",
      "predicting for file 06-05-31_480_510.wav\n",
      "predicting for file 06-09-20_420_450.wav\n",
      "predicting for file 06-11-22_1170_1200.wav\n",
      "predicting for file 07-05-02_870_900.wav\n",
      "predicting for file 07-11-14_1500_1530.wav\n",
      "predicting for file 06-10-18_840_870.wav\n",
      "predicting for file 08-01-15_1170_1200.wav\n",
      "predicting for file 06-05-03_570_600.wav\n",
      "predicting for file 08-01-23_1140_1170.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting for file 06-11-22_1680_1710.wav\n",
      "predicting for file 08-01-15_2220_2250.wav\n",
      "predicting for file 06-12-20_1260_1290.wav\n",
      "predicting for file 06-04-05_990_1020.wav\n",
      "predicting for file 06-09-06_870_900.wav\n",
      "predicting for file 08-01-23_1350_1380.wav\n",
      "predicting for file 06-10-04_2190_2220.wav\n",
      "predicting for file 06-04-12_1050_1080.wav\n",
      "predicting for file 06-04-12_2430_2460.wav\n",
      "predicting for file 06-11-29_1830_1860.wav\n",
      "predicting for file 07-02-21_2160_2190.wav\n",
      "predicting for file 07-06-20_570_600.wav\n",
      "predicting for file 08-01-30_480_510.wav\n",
      "predicting for file 07-09-17_1380_1410.wav\n",
      "predicting for file 06-05-10_510_540.wav\n",
      "predicting for file 07-12-19_1740_1770.wav\n",
      "predicting for file 07-03-28_780_810.wav\n",
      "predicting for file 06-09-13_1830_1860.wav\n",
      "predicting for file 08-01-15_960_990.wav\n",
      "predicting for file 07-03-28_1320_1350.wav\n",
      "predicting for file 08-01-23_2280_2310.wav\n",
      "predicting for file 07-02-28_450_480.wav\n",
      "predicting for file 08-02-06_2070_2100.wav\n",
      "predicting for file 07-05-30_240_270.wav\n",
      "predicting for file 08-01-23_1500_1530.wav\n",
      "predicting for file 06-12-13_240_270.wav\n",
      "predicting for file 07-11-14_1350_1380.wav\n",
      "predicting for file 06-09-13_1680_1710.wav\n",
      "predicting for file 07-09-17_600_630.wav\n",
      "predicting for file 07-04-11_1710_1740.wav\n",
      "predicting for file 06-11-08_1050_1080.wav\n",
      "predicting for file 07-01-31_1710_1740.wav\n",
      "predicting for file 06-05-17_1140_1170.wav\n",
      "predicting for file 08-02-06_870_900.wav\n",
      "predicting for file 06-11-15_390_420.wav\n",
      "predicting for file 06-09-13_1050_1080.wav\n",
      "predicting for file 07-02-14_2070_2100.wav\n",
      "predicting for file 06-11-22_1020_1050.wav\n",
      "predicting for file 07-02-21_1890_1920.wav\n",
      "predicting for file 06-09-20_1500_1530.wav\n",
      "predicting for file 07-02-28_2070_2100.wav\n",
      "predicting for file 06-09-06_120_150.wav\n",
      "predicting for file 06-09-13_1110_1140.wav\n",
      "predicting for file 07-01-17_1680_1710.wav\n",
      "predicting for file 06-05-24_1470_1500.wav\n",
      "predicting for file 06-05-10_2160_2190.wav\n",
      "predicting for file 08-01-30_1230_1260.wav\n",
      "predicting for file 06-11-15_240_270.wav\n",
      "predicting for file 06-12-20_1620_1650.wav\n",
      "predicting for file 06-10-18_1380_1410.wav\n",
      "predicting for file 08-01-15_1350_1380.wav\n",
      "predicting for file 07-03-28_990_1020.wav\n",
      "predicting for file 06-09-06_2160_2190.wav\n",
      "predicting for file 07-02-14_630_660.wav\n",
      "predicting for file 07-11-14_300_330.wav\n",
      "predicting for file 07-11-28_690_720.wav\n",
      "predicting for file 06-05-31_2280_2310.wav\n",
      "predicting for file 08-02-06_2040_2070.wav\n",
      "predicting for file 07-01-31_1980_2010.wav\n",
      "predicting for file 06-05-17_1800_1830.wav\n",
      "predicting for file 06-10-11_840_870.wav\n",
      "predicting for file 07-02-21_1590_1620.wav\n",
      "predicting for file 07-02-14_1500_1530.wav\n",
      "predicting for file 06-06-07_330_360.wav\n",
      "predicting for file 06-11-08_1560_1590.wav\n",
      "predicting for file 07-02-14_1170_1200.wav\n",
      "predicting for file 07-11-28_2220_2250.wav\n",
      "predicting for file 07-05-02_2010_2040.wav\n",
      "predicting for file 08-01-09_510_540.wav\n",
      "predicting for file 06-12-13_900_930.wav\n",
      "predicting for file 06-11-08_2160_2190.wav\n",
      "predicting for file 08-01-23_1260_1290.wav\n",
      "predicting for file 06-12-06_1770_1800.wav\n",
      "predicting for file 08-02-06_360_390.wav\n",
      "predicting for file 07-02-14_2010_2040.wav\n",
      "predicting for file 06-04-12_1890_1920.wav\n",
      "predicting for file 08-01-15_1470_1500.wav\n",
      "predicting for file 07-05-23_1860_1890.wav\n",
      "predicting for file 07-10-31_1770_1800.wav\n",
      "predicting for file 07-01-31_2130_2160.wav\n",
      "predicting for file 08-01-09_930_960.wav\n",
      "predicting for file 06-11-15_1620_1650.wav\n",
      "predicting for file 07-11-14_750_780.wav\n",
      "predicting for file 06-12-20_150_180.wav\n",
      "predicting for file 06-11-22_510_540.wav\n",
      "predicting for file 07-04-11_660_690.wav\n",
      "predicting for file 07-05-23_1350_1380.wav\n",
      "predicting for file 06-10-11_1380_1410.wav\n",
      "predicting for file 07-04-25_660_690.wav\n",
      "predicting for file 07-01-31_1590_1620.wav\n",
      "predicting for file 08-01-15_1920_1950.wav\n"
     ]
    }
   ],
   "source": [
    "predictions_list = []\n",
    "\n",
    "for file, cl in test_df[[\"0\", \"class\"]].values:\n",
    "    pred = predict_one_audio(file + \".wav\",cl)\n",
    "    \n",
    "    print(\"predicting for file {}\".format(file + \".wav\"))\n",
    "    predictions_list.append([i for sublist in pred for i in sublist])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"prediction\": predictions_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = test_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = test_df.join(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"avg\"] = final_df[\"prediction\"].apply(lambda x: np.mean(x))\n",
    "final_df[\"stdv\"] = final_df[\"prediction\"].apply(lambda x: np.std(x))\n",
    "final_df[\"positive\"] = final_df[\"prediction\"].apply(lambda x: sum(i >= 0.5 for i in x))\n",
    "final_df[\"negative\"] = final_df[\"prediction\"].apply(lambda x: sum(i < 0.5 for i in x))\n",
    "final_df[\"majority\"] = final_df.apply(lambda row: 1 if row[\"positive\"] >= row[\"negative\"] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>class</th>\n",
       "      <th>prediction</th>\n",
       "      <th>avg</th>\n",
       "      <th>stdv</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>majority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-05-10_2250_2280</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.99421763, 0.9979219, 0.99968576, 0.9975103,...</td>\n",
       "      <td>0.998532</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-09-13_2310_2340</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.94004524, 0.7154069, 0.98633236, 0.99207413...</td>\n",
       "      <td>0.605716</td>\n",
       "      <td>0.372241</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07-05-16_1440_1470</td>\n",
       "      <td>-5.9</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.009947419, 1.5199184e-06, 0.20875564, 0.000...</td>\n",
       "      <td>0.168271</td>\n",
       "      <td>0.319009</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>07-09-17_720_750</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.97027767, 0.48355103, 0.00017952919, 0.0045...</td>\n",
       "      <td>0.465564</td>\n",
       "      <td>0.401484</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06-11-08_1410_1440</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.027306736, 0.31755626, 0.8729861, 0.9844569...</td>\n",
       "      <td>0.793733</td>\n",
       "      <td>0.325576</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0    1  class  \\\n",
       "0  06-05-10_2250_2280  7.9      1   \n",
       "1  06-09-13_2310_2340  8.5      1   \n",
       "2  07-05-16_1440_1470 -5.9      0   \n",
       "3    07-09-17_720_750  1.1      1   \n",
       "4  06-11-08_1410_1440  7.4      1   \n",
       "\n",
       "                                          prediction       avg      stdv  \\\n",
       "0  [0.99421763, 0.9979219, 0.99968576, 0.9975103,...  0.998532  0.001665   \n",
       "1  [0.94004524, 0.7154069, 0.98633236, 0.99207413...  0.605716  0.372241   \n",
       "2  [0.009947419, 1.5199184e-06, 0.20875564, 0.000...  0.168271  0.319009   \n",
       "3  [0.97027767, 0.48355103, 0.00017952919, 0.0045...  0.465564  0.401484   \n",
       "4  [0.027306736, 0.31755626, 0.8729861, 0.9844569...  0.793733  0.325576   \n",
       "\n",
       "   positive  negative  majority  \n",
       "0        19         0         1  \n",
       "1        11         8         1  \n",
       "2         3        16         0  \n",
       "3         7        12         0  \n",
       "4        15         4         1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"./predictions_files.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelTr = np.vectorize(getLabels)\n",
    "pre_labels = labelTr(res)\n",
    "tru_labels = labelTr(y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         CN       0.60      1.00      0.75         3\n",
      "         CY       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.36      0.60      0.45         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "metrics = classification_report(tru_labels,pre_labels)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
